import os
import time
from argparse import ArgumentParser, FileType
from itertools import combinations
from dysto import *


parser = ArgumentParser(description="Generate distributional thesauri")
parser.add_argument('corpus', type=FileType('r'),
                    help="Corpus in conll format")

parser.add_argument('-W', '--stopwords', type=FileType('r'),
                    help="File containing a list of stopwords")
parser.add_argument('-x', '--exclude-tags', type=FileType('r'),
                    help="File containing a list of tags that will not be considered as context")
parser.add_argument('-t', '--tags', type=str, required=True,
                    help="Comma separated list of tags along with the number of " \
                    "lemma that will be used in each thesaurus (one per tag). " \
                    "Ex: dysto -t \"V:500,NC:2000,A:1000\" [...]")
parser.add_argument('-c', '--context-occurence', type=int, default=5,
                    help="Only consider contexts that occured at least n times. Default: 5")

size_limit = parser.add_mutually_exclusive_group()
size_limit.add_argument('-v', '--vocab-limit', type=int, default=None,
                        help="Cut the corpus when limit vocabulary size is reached")
size_limit.add_argument('-T', '--token-limit', type=int, default=None,
                        help="Cut the corpus when limit number of tokens is reached")

context_type = parser.add_mutually_exclusive_group(required=True)
context_type.add_argument('-b', '--bag-of-words', action='store_true', default=False,
                          help="Use bag of words context type")
context_type.add_argument('-p', '--positional', action='store_true', default=False,
                          help="Use positional context type")
context_type.add_argument('-d', '--dependency', action='store_true', default=False,
                          help="Use dependence relation context type")

parser.add_argument('-s', '--span', type=int, default=3,
                    help="Size of the span for bag of words contexts.")
parser.add_argument('-r', '--relations', type=FileType('r'),
                    help="File that describe the dependency relation to use.")
#parser.add_argument('-r', '--relations', type=FileType('r'),
                    #help="File containing syntactic relations to use in contexts")
parser.add_argument('-z', '--test', type=FileType('r'),
                          help="File containing internal tests")

parser.add_argument('-o', '--outdir', default='./dysto_output',
                    help="Output directory. Default: ./dysto_output")
parser.add_argument('-S', '--similarity', type=str, default='cosine',
                    choices=['cosine'],
                    help="Similarity scoring method. Default: cosine")
parser.add_argument('-B', '--backup', action='store_true', default=False,
                    help="Backup contexts in a file after parsing")
parser.add_argument('-V', '--verbose', action='store_true', default=False,
                    help="Print information about current process")


args = parser.parse_args()

# Prepare Output Directory
#
outdir = os.path.abspath(args.outdir)
if not os.path.isdir(outdir):
    os.mkdir(outdir)


# Prepare Session Directory
#
session_dir = os.path.join(outdir, time.strftime("%Y%m%d-%H%M%S"))
os.mkdir(session_dir)


# Setup Logging
#
log_file = open(os.path.join(session_dir, 'log'), 'w')

streams = [log_file]
if args.verbose:
    streams.append(sys.stdout)

logger = Logger(streams)


# Setup Backup Files
#
context_backup = open(os.path.join(session_dir, 'contexts'), 'w')
#vectors_backup = open(os.path.join(session_dir, 'vectors'), 'w')


logger.log("Command :\n\tdysto %s" % " ".join(sys.argv))
logger.log("Session directory :  %s" % session_dir)


logger.log("Using corpus :  %s" % args.corpus.name)

corpus = Corpus.from_stream(args.corpus)


# Compute Arguments
#
stopwords = []
if args.stopwords:
    logger.log("Using stopwords in :  %s" % args.stopwords.name)
    stopwords = read_stopwords(args.stopwords)

exclude_tags = []
if args.exclude_tags:
    logger.log("Exclude tags in :  %s" % args.exclude_tags.name)
    exclude_tags = read_stopwords(args.exclude_tags)

relations = []
if args.relations:
    logger.log("Use relations in :  %s" % args.relations.name)
    relations = read_relations(args.relations)


tags = {t: int(n) for t, n in [p.split(':') for p in args.tags.strip().split(',')]}

logger.log("Building thesauri for tags :")
for i in tags.items():
    logger.log("\t%s :  %i most frequent" % i)


# Generate Thesauri
#

if args.token_limit:
    corpus = corpus.limit_tokens(args.token_limit, logger=logger)

if stopwords:
    corpus = corpus.sanitize(stopwords, exclude_tags)

if args.vocab_limit:
    corpus = corpus.limit_vocabulary(args.vocab_limit)

if args.bag_of_words:
    contexts = corpus.bag_of_words_contexts(span=args.span, backup=context_backup)
elif args.positional:
    contexts = corpus.positional_contexts(span=args.span, backup=context_backup)
elif args.dependency:
    contexts = corpus.dependency_contexts(relations=relations, backup=context_backup)
else:
    sys.stderr.write('Only support bag of words contexts')
    exit(1)

vectors = contexts.context_vectors(tags, context_min=args.context_occurence, logger=logger) \
                  .ppmi()

thesauri = vectors.distributional_thesauri(sim=args.similarity, logger=logger)

for tag, thesaurus in thesauri.items():
    filename = os.path.join(session_dir, "thesaurus_%s" % tag)
    thesaurus.dump(open(filename, 'w'))

exit(0)

# vim: set filetype=python:
